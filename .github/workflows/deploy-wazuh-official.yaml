name: Deploy Wazuh (4.12.0, auto on commit)

on:
  push:
    branches: [ "main" ]
    paths:
      - ".github/workflows/**"
      - "wazuh/**"
      - "envs/**"
      - "**/*.yaml"
      - "**/*.yml"
      - "README.md"
  workflow_dispatch: {}

concurrency:
  group: deploy-wazuh
  cancel-in-progress: true

jobs:
  deploy:
    runs-on: self-hosted
    env:
      NS: wazuh
      WAZUH_TAG: v4.12.0
      DASH_PORT: 32563

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Verify kubeconfig secret exists
        run: |
          if [ -z "${{ secrets.KUBE_CONFIG_B64 }}" ]; then
            echo "::error::Missing KUBE_CONFIG_B64 secret (Settings → Secrets → Actions)."
            exit 1
          fi

      - name: Setup kubectl (pin close to cluster)
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.30.14'

      - name: Install openssl and jq
        run: |
          set -euxo pipefail
          sudo apt-get update
          sudo apt-get install -y openssl jq

      # --- kubeconfig from base64 (authoritative) ---
      - name: Write kubeconfig
        env:
          KUBE_CONFIG_B64: ${{ secrets.KUBE_CONFIG_B64 }}
        run: |
          set -euo pipefail
          mkdir -p "$HOME/.kube"
          out="$HOME/.kube/config"
          echo "${KUBE_CONFIG_B64}" | base64 -d > "$out"
          tr -d '\r' < "$out" > "$out.tmp" && mv "$out.tmp" "$out"
          chmod 600 "$out"
          echo "KUBECONFIG=$out" >> "$GITHUB_ENV"
          kubectl config get-contexts >/dev/null

      - name: Show cluster and nodes
        run: |
          set -euxo pipefail
          echo "Context: $(kubectl config current-context || true)"
          echo "API server: $(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}' || true)"
          kubectl version --client=true -o yaml || kubectl version --client=true || true
          kubectl get nodes -o wide

      - name: Runner breadcrumb on Ronin
        run: |
          set -euxo pipefail
          sudo mkdir -p /opt/actions-runner/job-logs
          echo "$(date -Is) job=${GITHUB_JOB} run=${GITHUB_RUN_ID} sha=${GITHUB_SHA} actor=${GITHUB_ACTOR}" \
            | sudo tee -a /opt/actions-runner/job-logs/wazuh-actions.log

      - name: Clone wazuh-kubernetes v4.12.0
        run: |
          set -euxo pipefail
          rm -rf wazuh-kubernetes
          git clone https://github.com/wazuh/wazuh-kubernetes.git -b "$WAZUH_TAG" --depth=1
          ls -la wazuh-kubernetes

      - name: Generate certs
        working-directory: ./wazuh-kubernetes
        run: |
          set -euxo pipefail
          bash wazuh/certs/indexer_cluster/generate_certs.sh
          bash wazuh/certs/dashboard_http/generate_certs.sh
          echo "Generated cert files (sample):"
          find wazuh/certs -type f \( -name "*.pem" -o -name "*.key" \) | head -n 20

      - name: Dry run build (client-side with kubectl kustomize)
        working-directory: ./wazuh-kubernetes
        run: |
          set -euxo pipefail
          kubectl kustomize envs/local-env | head -n 60

      - name: Ensure wazuh namespace exists
        run: |
          set -euxo pipefail
          kubectl create namespace "$NS" --dry-run=client -o yaml | kubectl apply -f -

      # --- IMPORTANT: make wazuh-storage match your default provisioner (e.g., rancher.io/local-path) ---
      - name: Ensure 'wazuh-storage' StorageClass (alias of default)
        run: |
          set -euxo pipefail
          # Find default SC; if none annotated default, pick the first
          DEF_SC=$(kubectl get storageclass -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.metadata.annotations.storageclass\.kubernetes\.io/is-default-class}{"\n"}{end}' \
                   | awk '$2=="true"{print $1}' | head -n1)
          if [ -z "$DEF_SC" ]; then
            DEF_SC=$(kubectl get storageclass -o jsonpath='{.items[0].metadata.name}')
          fi
          echo "Default StorageClass: ${DEF_SC}"

          DEF_PROV=$(kubectl get sc "$DEF_SC" -o jsonpath='{.provisioner}')
          echo "Default provisioner: ${DEF_PROV}"

          # If wazuh-storage exists but with wrong provisioner, replace it
          if kubectl get sc wazuh-storage >/dev/null 2>&1; then
            CUR_PROV=$(kubectl get sc wazuh-storage -o jsonpath='{.provisioner}')
            if [ "$CUR_PROV" != "$DEF_PROV" ]; then
              echo "Replacing existing wazuh-storage (provisioner=$CUR_PROV) with $DEF_PROV"
              kubectl delete sc wazuh-storage
            fi
          fi

          # Create (or re-create) wazuh-storage alias
          if ! kubectl get sc wazuh-storage >/dev/null 2>&1; then
            cat > /tmp/wazuh-storage.yaml <<EOF
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: wazuh-storage
          provisioner: ${DEF_PROV}
          reclaimPolicy: Delete
          volumeBindingMode: WaitForFirstConsumer
          allowVolumeExpansion: true
          EOF
            kubectl apply -f /tmp/wazuh-storage.yaml
          fi

          echo "StorageClasses:"
          kubectl get storageclass -o wide

      - name: Server dry-run apply (catches RBAC/schema)
        working-directory: ./wazuh-kubernetes
        run: |
          set -euxo pipefail
          kubectl apply -k envs/local-env/ --dry-run=server

      - name: Apply manifests (local-env)
        working-directory: ./wazuh-kubernetes
        run: |
          set -euxo pipefail
          kubectl apply -k envs/local-env/

      - name: Wait for PVCs to bind (wazuh namespace)
        run: |
          set -euxo pipefail
          if kubectl -n "$NS" get pvc --no-headers 2>/dev/null | grep -q .; then
            echo "Waiting for all PVCs in namespace '$NS' to be Bound..."
            timeout 600 bash -c 'until [ -z "$(kubectl -n '"$NS"' get pvc --no-headers | awk '"'"'{print $2}'"'"' | grep -v Bound)" ]; do kubectl -n '"$NS"' get pvc; sleep 5; done'
          else
            echo "No PVCs found (yet)."
          fi
          kubectl -n "$NS" get pvc -o wide || true

      - name: Wait for pods to be ready
        run: |
          set -euxo pipefail
          kubectl -n "$NS" wait --for=condition=Ready pod -l app=wazuh-indexer --timeout=900s || true
          kubectl -n "$NS" wait --for=condition=Ready pod -l app=wazuh-manager --timeout=900s || true
          kubectl -n "$NS" wait --for=condition=Ready pod -l app=wazuh-dashboard --timeout=900s || true
          kubectl -n "$NS" get pods -o wide || true

      - name: Expose dashboard on NodePort
        env:
          DASH_PORT: ${{ env.DASH_PORT }}
        run: |
          set -euxo pipefail
          SVC=$(kubectl -n "$NS" get svc -l app=wazuh-dashboard -o jsonpath='{.items[0].metadata.name}' || true)
          if [ -z "$SVC" ]; then
            echo "::warning::Dashboard Service not found; skipping patch."
            exit 0
          fi
          cat > /tmp/patch.json <<'EOF'
          {
            "spec": {
              "type": "NodePort",
              "ports": [{
                "name": "https",
                "port": 443,
                "targetPort": 443,
                "protocol": "TCP",
                "nodePort": __NODEPORT__
              }]
            }
          }
          EOF
          sed -i "s/__NODEPORT__/${DASH_PORT}/" /tmp/patch.json
          kubectl -n "$NS" patch svc "$SVC" --type=merge -p "$(cat /tmp/patch.json)"
          kubectl -n "$NS" get svc "$SVC" -o wide
          echo "URL: https://<any-node-ip>:${DASH_PORT}"

      - name: Quick status
        run: |
          set -euxo pipefail
          echo "=== Pods ==="
          kubectl -n "$NS" get pods -o wide || true
          echo "=== Services ==="
          kubectl -n "$NS" get svc -o wide || true
          echo "=== PVCs ==="
          kubectl -n "$NS" get pvc -o wide || true
          echo "=== Deployments/StatefulSets ==="
          kubectl -n "$NS" get statefulsets,deploy -o wide || true

      - name: Collect diagnostics
        if: always()
        run: |
          set -euxo pipefail
          mkdir -p diag
          kubectl -n "$NS" get all -o wide > diag/get-all.txt 2>&1 || true
          kubectl -n "$NS" get events --sort-by=.lastTimestamp > diag/events.txt 2>&1 || true
          kubectl -n "$NS" get pods -o yaml > diag/pods.yaml 2>&1 || true
          kubectl -n "$NS" get pvc -o yaml > diag/pvc.yaml 2>&1 || true
          kubectl get storageclass -o yaml > diag/storageclass.yaml 2>&1 || true
          kubectl -n "$NS" get svc -o yaml > diag/svc.yaml 2>&1 || true
          for pod in $(kubectl -n "$NS" get pods -o name 2>/dev/null); do
            pn=${pod#pod/}
            kubectl -n "$NS" logs "$pod" --all-containers=true --prefix=true > "diag/logs-${pn}.txt" 2>&1 || true
            kubectl -n "$NS" describe "$pod" > "diag/describe-${pn}.txt" 2>&1 || true
          done

      - name: Upload diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: wazuh-diagnostics-${{ github.run_id }}
          path: diag/**

      - name: Health check and URL hint
        run: |
          set -euxo pipefail
          kubectl -n "$NS" get pods -o wide || true
          NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[0].address}' 2>/dev/null || echo "<node-ip>")
          echo "Wazuh Dashboard URL: https://${NODE_IP}:${DASH_PORT}"
          echo "Accept the self-signed certificate warning."
