name: Deploy Wazuh (4.12.0, auto on commit)

on:
  push:
    branches: [ "main" ]
    paths:
      - ".github/workflows/**"
      - "wazuh/**"
      - "envs/**"
      - "**/*.yaml"
      - "**/*.yml"
      - "README.md"
  workflow_dispatch: {}

concurrency:
  group: deploy-wazuh
  cancel-in-progress: true

jobs:
  deploy:
    runs-on: self-hosted
    env:
      NS: wazuh
      WAZUH_TAG: v4.12.0

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Verify kubeconfig secret exists
        run: |
          if [ -z "${{ secrets.KUBE_CONFIG_B64 }}" ]; then
            echo "::error::Missing KUBE_CONFIG_B64 secret (Settings → Secrets → Actions)."
            exit 1
          fi

      - name: Setup kubectl (pin to your cluster version)
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.30.14'

      - name: Install openssl, jq, yq
        run: |
          set -euxo pipefail
          sudo apt-get update
          sudo apt-get install -y openssl jq
          sudo curl -L https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -o /usr/local/bin/yq
          sudo chmod +x /usr/local/bin/yq
          yq --version

      - name: Write kubeconfig
        env:
          KUBE_CONFIG_B64: ${{ secrets.KUBE_CONFIG_B64 }}
        run: |
          set -euo pipefail
          mkdir -p "$HOME/.kube"
          out="$HOME/.kube/config"
          echo "${KUBE_CONFIG_B64}" | base64 -d > "$out"
          tr -d '\r' < "$out" > "$out.tmp" && mv "$out.tmp" "$out"
          chmod 600 "$out"
          echo "KUBECONFIG=$out" >> "$GITHUB_ENV"
          kubectl config get-contexts >/dev/null

      - name: Show cluster and nodes
        run: |
          set -euxo pipefail
          echo "Context: $(kubectl config current-context || true)"
          echo "API server: $(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}' || true)"
          kubectl version --client=true -o yaml || kubectl version --client=true || true
          kubectl get nodes -o wide

      - name: Runner breadcrumb on Ronin
        run: |
          set -euxo pipefail
          sudo mkdir -p /opt/actions-runner/job-logs
          echo "$(date -Is) job=${GITHUB_JOB} run=${GITHUB_RUN_ID} sha=${GITHUB_SHA} actor=${GITHUB_ACTOR}" \
            | sudo tee -a /opt/actions-runner/job-logs/wazuh-actions.log

      - name: Clone wazuh-kubernetes ${{ env.WAZUH_TAG }}
        run: |
          set -euxo pipefail
          rm -rf wazuh-kubernetes
          git clone https://github.com/wazuh/wazuh-kubernetes.git -b "$WAZUH_TAG" --depth=1
          ls -la wazuh-kubernetes

      - name: Generate certs
        working-directory: ./wazuh-kubernetes
        run: |
          set -euxo pipefail
          bash wazuh/certs/indexer_cluster/generate_certs.sh
          bash wazuh/certs/dashboard_http/generate_certs.sh
          echo "Generated cert files (sample):"
          find wazuh/certs -type f \( -name "*.pem" -o -name "*.key" \) | head -n 20

      - name: Ensure wazuh namespace exists
        run: |
          set -euxo pipefail
          kubectl create namespace "$NS" --dry-run=client -o yaml | kubectl apply -f -

      - name: Ensure 'wazuh-storage' StorageClass (alias of dynamic provisioner)
        run: |
          set -euxo pipefail
          if kubectl get sc local-path >/dev/null 2>&1; then
            TARGET_SC="local-path"
          else
            TARGET_SC=$(kubectl get sc -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.metadata.annotations.storageclass\.kubernetes\.io/is-default-class}{"\t"}{.provisioner}{"\n"}{end}' \
              | awk '$2=="true" && $3!="kubernetes.io/no-provisioner"{print $1}' | head -n1)
            if [ -z "${TARGET_SC}" ]; then
              TARGET_SC=$(kubectl get sc -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.provisioner}{"\n"}{end}' \
                | awk '$2!="kubernetes.io/no-provisioner"{print $1}' | head -n1)
            fi
          fi
          if [ -z "${TARGET_SC:-}" ]; then
            echo "::error::No dynamic StorageClass found (local-path or other). Install a dynamic provisioner."
            exit 1
          fi
          TARGET_PROV=$(kubectl get sc "$TARGET_SC" -o jsonpath='{.provisioner}')
          echo "Using dynamic SC '${TARGET_SC}' with provisioner '${TARGET_PROV}' for wazuh-storage"
          if kubectl get sc wazuh-storage >/dev/null 2>&1; then
            CUR_PROV=$(kubectl get sc wazuh-storage -o jsonpath='{.provisioner}')
            if [ "$CUR_PROV" != "$TARGET_PROV" ]; then
              kubectl delete sc wazuh-storage
            fi
          fi
          if ! kubectl get sc wazuh-storage >/dev/null 2>&1; then
            cat > /tmp/wazuh-storage.yaml <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: wazuh-storage
provisioner: ${TARGET_PROV}
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
EOF
            kubectl apply -f /tmp/wazuh-storage.yaml
          fi
          kubectl get storageclass -o wide

      - name: Build manifests (strip StorageClass)
        working-directory: ./wazuh-kubernetes
        run: |
          set -euxo pipefail
          kubectl kustomize envs/local-env > /tmp/wazuh-all.yaml
          yq 'select(.kind != "StorageClass")' /tmp/wazuh-all.yaml > /tmp/wazuh-no-sc.yaml
          echo "First docs after filtering:"
          head -n 80 /tmp/wazuh-no-sc.yaml || true

      - name: Server dry-run apply (validate)
        run: |
          set -euxo pipefail
          kubectl apply -f /tmp/wazuh-no-sc.yaml --dry-run=server

      - name: Apply manifests (no StorageClass)
        run: |
          set -euxo pipefail
          kubectl apply -f /tmp/wazuh-no-sc.yaml

      - name: Wait for PVCs to bind (initial)
        run: |
          set -euxo pipefail
          if kubectl -n "$NS" get pvc --no-headers 2>/dev/null | grep -q .; then
            echo "Waiting up to 5 minutes for all PVCs in '$NS' to be Bound..."
            timeout 300 bash -c 'until [ -z "$(kubectl -n '"$NS"' get pvc --no-headers | awk '"'"'{print $2}'"'"' | grep -v Bound)" ]; do kubectl -n '"$NS"' get pvc; sleep 5; done' || true
          else
            echo "No PVCs found (yet)."
          fi
          kubectl -n "$NS" get pvc -o wide || true

      - name: Auto-recover unbound PVCs
        run: |
          set -euxo pipefail
          UNBOUND=$(kubectl -n "$NS" get pvc -o json | jq -r '.items[] | select(.status.phase!="Bound") | .metadata.name' || true)
          if [ -n "$UNBOUND" ]; then
            echo "Unbound PVCs detected: $UNBOUND"
            kubectl -n "$NS" scale statefulset wazuh-indexer --replicas=0 || true
            kubectl -n "$NS" scale statefulset wazuh-manager-master --replicas=0 || true
            kubectl -n "$NS" scale statefulset wazuh-manager-worker --replicas=0 || true
            echo "$UNBOUND" | xargs -r -n1 kubectl -n "$NS" delete pvc --ignore-not-found
            sleep 5
            kubectl -n "$NS" scale statefulset wazuh-indexer --replicas=1 || true
            kubectl -n "$NS" scale statefulset wazuh-manager-master --replicas=1 || true
            kubectl -n "$NS" scale statefulset wazuh-manager-worker --replicas=1 || true
          else
            echo "No unbound PVCs to recover."
          fi

      - name: Wait for PVCs to bind (post-recovery)
        run: |
          set -euxo pipefail
          if kubectl -n "$NS" get pvc --no-headers 2>/dev/null | grep -q .; then
            echo "Waiting up to 5 minutes for all PVCs (post-recovery) to be Bound..."
            timeout 300 bash -c 'until [ -z "$(kubectl -n '"$NS"' get pvc --no-headers | awk '"'"'{print $2}'"'"' | grep -v Bound)" ]; do kubectl -n '"$NS"' get pvc; sleep 5; done' || true
          fi
          kubectl -n "$NS" get pvc -o wide || true

      - name: Wait for pods to be ready
        run: |
          set -euxo pipefail
          kubectl -n "$NS" wait --for=condition=Ready pod -l app=wazuh-indexer --timeout=900s || true
          kubectl -n "$NS" wait --for=condition=Ready pod -l app=wazuh-manager --timeout=900s || true
          kubectl -n "$NS" wait --for=condition=Ready pod -l app=wazuh-dashboard --timeout=900s || true
          kubectl -n "$NS" get pods -o wide || true

      # === changed: let Kubernetes assign a random nodePort ===
      - name: Expose dashboard on NodePort (auto-assign)
        run: |
          set -euxo pipefail
          SVC=$(kubectl -n "$NS" get svc -l app=wazuh-dashboard -o jsonpath='{.items[0].metadata.name}' || true)
          if [ -z "$SVC" ]; then
            echo "::warning::Dashboard Service not found; skipping patch."
            exit 0
          fi

          # Switch to NodePort (if coming from LoadBalancer/ClusterIP, K8s will allocate a nodePort automatically)
          kubectl -n "$NS" patch svc "$SVC" -p '{"spec":{"type":"NodePort"}}'

          # If a fixed nodePort was already set previously, remove it to force re-allocation
          kubectl -n "$NS" patch svc "$SVC" --type='json' -p='[{"op":"remove","path":"/spec/ports/0/nodePort"}]' || true

          # Read the assigned nodePort
          ASSIGNED_PORT=$(kubectl -n "$NS" get svc "$SVC" -o jsonpath='{.spec.ports[0].nodePort}')
          echo "Assigned NodePort: ${ASSIGNED_PORT}"
          kubectl -n "$NS" get svc "$SVC" -o wide
          echo "URL: https://<any-node-ip>:${ASSIGNED_PORT}"

      - name: Quick status
        run: |
          set -euxo pipefail
          echo "=== Pods ==="
          kubectl -n "$NS" get pods -o wide || true
          echo "=== Services ==="
          kubectl -n "$NS" get svc -o wide || true
          echo "=== PVCs ==="
          kubectl -n "$NS" get pvc -o wide || true
          echo "=== Deployments/StatefulSets ==="
          kubectl -n "$NS" get statefulsets,deploy -o wide || true

      - name: Collect diagnostics
        if: always()
        run: |
          set -euxo pipefail
          mkdir -p diag
          kubectl -n "$NS" get all -o wide > diag/get-all.txt 2>&1 || true
          kubectl -n "$NS" get events --sort-by=.lastTimestamp > diag/events.txt 2>&1 || true
          kubectl -n "$NS" get pods -o yaml > diag/pods.yaml 2>&1 || true
          kubectl -n "$NS" get pvc -o yaml > diag/pvc.yaml 2>&1 || true
          kubectl get storageclass -o yaml > diag/storageclass.yaml 2>&1 || true
          kubectl -n "$NS" get svc -o yaml > diag/svc.yaml 2>&1 || true
          for pod in $(kubectl -n "$NS" get pods -o name 2>/dev/null); do
            pn=${pod#pod/}
            kubectl -n "$NS" logs "$pod" --all-containers=true --prefix=true > "diag/logs-${pn}.txt" 2>&1 || true
            kubectl -n "$NS" describe "$pod" > "diag/describe-${pn}.txt" 2>&1 || true
          done

      - name: Upload diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: wazuh-diagnostics-${{ github.run_id }}
          path: diag/**

      - name: Health check and URL hint
        run: |
          set -euxo pipefail
          kubectl -n "$NS" get pods -o wide || true
          SVC=$(kubectl -n "$NS" get svc -l app=wazuh-dashboard -o jsonpath='{.items[0].metadata.name}' || echo "")
          PORT=$( [ -n "$SVC" ] && kubectl -n "$NS" get svc "$SVC" -o jsonpath='{.spec.ports[0].nodePort}' || echo "<port>" )
          NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[0].address}' 2>/dev/null || echo "<node-ip>")
          echo "Wazuh Dashboard URL: https://${NODE_IP}:${PORT}"
          echo "Accept the self-signed certificate warning."
