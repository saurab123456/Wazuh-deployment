name: Deploy Wazuh to Ronin (Self-Hosted)

on:
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - "wazuh-kubernetes/**"
      - "k8s/**"
      - ".github/workflows/deploy-wazuh-ronin-selfhosted.yaml"

jobs:
  deploy:
    runs-on: [self-hosted, linux, ronin, k8s, control-plane]
    timeout-minutes: 30

    env:
      # --- YOUR SETTINGS ---
      NS: wazuh
      WAZUH_TAG: v4.12.0
      DASH_PORT: 32563
      # kubeconfig gets written here:
      KUBECONFIG: ${{ github.workspace }}/kubeconfig.yaml

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Verify runner connection
        run: |
          echo "Host: $(hostname)"
          echo "Time: $(date)"
          uname -a

      - name: Verify kubectl availability
        run: |
          if command -v kubectl >/dev/null 2>&1; then
            echo "✅ kubectl present: $(kubectl version --client=true | head -n 1)"
          else
            echo "⚠️ installing kubectl..."
            KVER="$(curl -sS https://dl.k8s.io/release/stable.txt)"
            curl -sSLo kubectl "https://dl.k8s.io/release/${KVER}/bin/linux/amd64/kubectl"
            chmod +x kubectl && sudo mv kubectl /usr/local/bin/
            kubectl version --client=true | head -n 1
          fi

      - name: Install yq (for YAML transforms)
        run: |
          curl -sSL -o /tmp/yq.tar.gz https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64.tar.gz
          tar -xzf /tmp/yq.tar.gz -C /tmp
          sudo mv /tmp/yq_linux_amd64 /usr/local/bin/yq
          sudo chmod +x /usr/local/bin/yq
          yq --version

      # kubeconfig from base64 (authoritative)
      - name: Write kubeconfig
        env:
          KUBE_CONFIG_B64: ${{ secrets.KUBE_CONFIG_B64 }}
        run: |
          if [ -z "${KUBE_CONFIG_B64}" ]; then
            echo "::error::Secret KUBE_CONFIG_B64 is missing"; exit 1
          fi
          echo "${KUBE_CONFIG_B64}" | base64 -d > "$KUBECONFIG"
          echo "First lines of kubeconfig:" && head -n 5 "$KUBECONFIG" || true

      - name: Sanity check cluster
        run: |
          kubectl cluster-info
          kubectl get nodes -o wide

      - name: Ensure namespace
        run: |
          kubectl create namespace "$NS" --dry-run=client -o yaml | kubectl apply -f -

      # Make wazuh-storage an alias of a DYNAMIC provisioner (prefers rancher local-path)
      - name: Ensure 'wazuh-storage' StorageClass (alias of dynamic provisioner)
        run: |
          set -euo pipefail
          # Prefer rancher local-path if present
          if kubectl get sc local-path >/dev/null 2>&1; then
            TARGET_SC="local-path"
          else
            # pick a default SC that is NOT 'kubernetes.io/no-provisioner'
            TARGET_SC=$(kubectl get sc -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.metadata.annotations.storageclass\.kubernetes\.io/is-default-class}{"\t"}{.provisioner}{"\n"}{end}' \
              | awk '$2=="true" && $3!="kubernetes.io/no-provisioner"{print $1}' | head -n1)
            # fallback: first SC that isn’t no-provisioner
            if [ -z "${TARGET_SC}" ]; then
              TARGET_SC=$(kubectl get sc -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.provisioner}{"\n"}{end}' \
                | awk '$2!="kubernetes.io/no-provisioner"{print $1}' | head -n1)
            fi
          fi

          if [ -z "${TARGET_SC:-}" ]; then
            echo "::error::No dynamic StorageClass found (local-path or other). Install a dynamic provisioner."
            exit 1
          fi

          TARGET_PROV=$(kubectl get sc "$TARGET_SC" -o jsonpath='{.provisioner}')
          echo "Using dynamic SC '${TARGET_SC}' with provisioner '${TARGET_PROV}' for wazuh-storage"

          # If wazuh-storage exists with a different provisioner, replace it cleanly
          if kubectl get sc wazuh-storage >/dev/null 2>&1; then
            CUR_PROV=$(kubectl get sc wazuh-storage -o jsonpath='{.provisioner}')
            if [ "$CUR_PROV" != "$TARGET_PROV" ]; then
              kubectl delete sc wazuh-storage
            fi
          fi

          if ! kubectl get sc wazuh-storage >/dev/null 2>&1; then
            cat > /tmp/wazuh-storage.yaml <<EOF
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: wazuh-storage
          provisioner: ${TARGET_PROV}
          reclaimPolicy: Delete
          volumeBindingMode: WaitForFirstConsumer
          allowVolumeExpansion: true
          EOF
            kubectl apply -f /tmp/wazuh-storage.yaml
          fi

          kubectl get storageclass -o wide

      # Build manifests and drop any StorageClass objects to avoid immutability conflicts
      - name: Build manifests (strip StorageClass)
        if: ${{ hashFiles('wazuh-kubernetes/**') != '' }}
        working-directory: ./wazuh-kubernetes
        run: |
          set -euo pipefail
          mkdir -p ../_rendered
          # Render everything if kustomization exists, else just copy yaml files
          if [ -f kustomization.yaml ]; then
            kubectl kustomize . > ../_rendered/all.yaml
          else
            cat $(git ls-files '*.yml' '*.yaml') > ../_rendered/all.yaml
          fi
          # Remove any StorageClass objects in the render to avoid clashes
          yq 'select(.kind != "StorageClass")' ../_rendered/all.yaml > ../_rendered/filtered.yaml
          echo "Rendered to ../_rendered/filtered.yaml"

      # Apply manifests: prefer rendered set; otherwise try k8s/manifests if present
      - name: Apply manifests
        run: |
          set -euo pipefail
          if [ -f "_rendered/filtered.yaml" ]; then
            kubectl apply -f _rendered/filtered.yaml
          elif [ -d "k8s/manifests" ]; then
            kubectl apply -f k8s/manifests/
          else
            echo "::error::No manifests found (wazuh-kubernetes/ or k8s/manifests/)"; exit 1
          fi

      - name: Wait for core pods (manager/indexer/dashboard)
        run: |
          set -euxo pipefail
          kubectl -n "$NS" get pods -o wide || true
          kubectl -n "$NS" wait --for=condition=Ready pod -l app=wazuh-manager --timeout=900s || true
          kubectl -n "$NS" wait --for=condition=Ready pod -l app=wazuh-indexer --timeout=900s || true
          kubectl -n "$NS" wait --for=condition=Ready pod -l app=wazuh-dashboard --timeout=900s || true
          kubectl -n "$NS" get pods -o wide || true

      # Expose dashboard on NodePort (let K8s choose the port) and print URL
      - name: Expose dashboard on NodePort (auto-assign)
        run: |
          set -euo pipefail
          SVC=$(kubectl -n "$NS" get svc -l app=wazuh-dashboard -o jsonpath='{.items[0].metadata.name}' || true)
          if [ -z "$SVC" ]; then
            echo "::warning::Dashboard Service not found; skipping patch."
            exit 0
          fi

          # Switch to NodePort and let Kubernetes allocate nodePort
          kubectl -n "$NS" patch svc "$SVC" -p '{"spec":{"type":"NodePort"}}'
          # If a fixed nodePort was previously set, remove it to force re-allocation
          kubectl -n "$NS" patch svc "$SVC" --type='json' -p='[{"op":"remove","path":"/spec/ports/0/nodePort"}]' || true

          ASSIGNED_PORT=$(kubectl -n "$NS" get svc "$SVC" -o jsonpath='{.spec.ports[0].nodePort}')
          NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[0].address}' 2>/dev/null || echo "<node-ip>")
          echo "Assigned NodePort: ${ASSIGNED_PORT}"
          echo "Wazuh Dashboard URL (auto): https://${NODE_IP}:${ASSIGNED_PORT}"
          echo "Wazuh Dashboard URL (fixed): https://${NODE_IP}:${DASH_PORT} (if you later force it)"

      - name: Quick status
        run: |
          kubectl -n "$NS" get pods -o wide || true
          kubectl -n "$NS" get svc || true
