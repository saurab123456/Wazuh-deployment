name: 03) Enforce Passwordless Auth (Managers)

on:
  workflow_dispatch: {}

permissions:
  contents: read

concurrency:
  group: passwordless-${{ github.ref }}
  cancel-in-progress: true

jobs:
  passwordless:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    runs-on: [self-hosted, linux, ronin, k8s, control-plane]
    timeout-minutes: 45
    env:
      NS: wazuh
      KUBECONFIG: ${{ github.workspace }}/kubeconfig.yaml
      RAW_KCFG: ${{ secrets.KUBECONFIG }}
      KCFG_B64: ${{ secrets.KUBECONFIG_B64 }}
      KCFG_B64_ALT: ${{ secrets.KUBE_CONFIG_B64 }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Mask secrets (if present)
        shell: bash
        run: |
          for v in RAW_KCFG KCFG_B64 KCFG_B64_ALT; do
            val="${!v:-}"
            if [ -n "$val" ]; then
              echo "::add-mask::$val"
            fi
          done

      - name: Ensure CLIs (kubectl + yq) no sudo
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$HOME/.local/bin"
          if ! command -v kubectl >/dev/null 2>&1; then
            curl -fsSL -o "$HOME/.local/bin/kubectl" "https://dl.k8s.io/release/$(curl -fsSL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            chmod +x "$HOME/.local/bin/kubectl"
          fi
          if ! command -v yq >/dev/null 2>&1; then
            curl -fsSL -o "$HOME/.local/bin/yq" "https://github.com/mikefarah/yq/releases/download/v4.44.3/yq_linux_amd64"
            chmod +x "$HOME/.local/bin/yq"
          fi
          echo "$HOME/.local/bin" >> "$GITHUB_PATH"

      - name: Write kubeconfig from secrets
        shell: bash
        run: |
          set -euo pipefail
          if [ -n "${RAW_KCFG:-}" ]; then
            printf '%s' "$RAW_KCFG" > "$KUBECONFIG"
          elif [ -n "${KCFG_B64:-}" ]; then
            printf '%s' "$KCFG_B64" | base64 -d > "$KUBECONFIG"
          elif [ -n "${KCFG_B64_ALT:-}" ]; then
            printf '%s' "$KCFG_B64_ALT" | base64 -d > "$KUBECONFIG"
          else
            echo "::error::No kubeconfig secret found. Provide one of: KUBECONFIG (raw), KUBECONFIG_B64 (b64), or KUBE_CONFIG_B64 (b64)."
            exit 1
          fi
          chmod 600 "$KUBECONFIG"

      - name: Sanity check cluster connectivity
        shell: bash
        env:
          KUBECONFIG: ${{ env.KUBECONFIG }}
        run: |
          set -euo pipefail
          kubectl version --short
          kubectl get nodes -o wide
          kubectl -n "$NS" get pods -l app=wazuh-manager -o wide

      - name: Patch manager ossec.conf in ConfigMaps to passwordless (1515)
        shell: bash
        env:
          KUBECONFIG: ${{ env.KUBECONFIG }}
        run: |
          set -euo pipefail
          CM_NAMES="$(
            { kubectl -n "$NS" get sts -l app=wazuh-manager \
                -o jsonpath='{range .items[*].spec.template.spec.volumes[*]}{.configMap.name}{"\n"}{end}';
              kubectl -n "$NS" get pods -l app=wazuh-manager \
                -o jsonpath='{range .items[*].spec.volumes[*]}{.configMap.name}{"\n"}{end}';
            } 2>/dev/null | sort -u | sed '/^$/d' || true
          )"
          if [ -z "$CM_NAMES" ]; then
            echo "::warning::No manager ConfigMaps found that hold ossec.conf (skipping CM patch)"
          else
            for CM in $CM_NAMES; do
              echo ">> Processing ConfigMap: $CM"
              kubectl -n "$NS" get cm "$CM" -o yaml > "/tmp/${CM}.yaml"
              OSSEC="$(yq -r '.data."ossec.conf"' "/tmp/${CM}.yaml" || true)"
              if [ -z "$OSSEC" ] || [ "$OSSEC" = "null" ]; then
                echo "::warning::${CM} has no data.ossec.conf; skipping"
                continue
              fi
              yq -i '
                .data."ossec.conf" =
                  (.data."ossec.conf"
                    | sub("(?s)(<auth>)(.*?)(</auth>)";
                          capture("(?s)(?<open><auth>)(?<body>.*?)(?<close></auth>)") as $m
                          | ($m.body
                              | sub("<use_password>[^<]+</use_password>"; "<use_password>no</use_password>")
                              | sub("<port>[^<]+</port>"; "<port>1515</port>")
                              | sub("(?s)[[:space:]]*<password>[^<]*</password>[[:space:]]*"; "")
                            ) as $nb
                          | "\($m.open)\($nb)\($m.close)"
                    )
                  )
              ' "/tmp/${CM}.yaml"
              kubectl -n "$NS" apply -f "/tmp/${CM}.yaml"
            done
          fi

      - name: Force passwordless in running pods & purge authd.pass
        shell: bash
        env:
          KUBECONFIG: ${{ env.KUBECONFIG }}
        run: |
          set -euo pipefail
          PODS="$(kubectl -n "$NS" get pods -l app=wazuh-manager -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}')"
          if [ -z "$PODS" ]; then
            echo "::error::No wazuh-manager pods found"; exit 1
          fi
          for P in $PODS; do
            echo "Patching $P ..."
            kubectl -n "$NS" exec "$P" -c wazuh-manager -- sh -lc '
              set -e
              f=/var/ossec/etc/ossec.conf
              cp "$f" "${f}.bak.$(date +%s)" || true
              sed -i -E "/<auth>/,/<\\/auth>/ s|<port>[0-9]+</port>|<port>1515</port>|" "$f"
              sed -i -E "/<auth>/,/<\\/auth>/ s|<use_password>[^<]+</use_password>|<use_password>no</use_password>|" "$f"
              awk '"'"'BEGIN{inauth=0} /<auth>/{inauth=1} inauth && /<password>.*<\/password>/{next} /<\/auth>/{inauth=0} {print}'"'"' "$f" > "$f.tmp" && mv "$f.tmp" "$f"
              rm -f /var/ossec/etc/authd.pass || true
              echo "----- <auth> after patch (pod $(hostname)) -----"
              sed -n "/<auth>/,/<\\/auth>/p" "$f"
            '
          done

      - name: Rollout restart managers
        shell: bash
        env:
          KUBECONFIG: ${{ env.KUBECONFIG }}
        run: |
          set -e
          # Restart common StatefulSet names if they exist; ignore if not present
          for s in wazuh-manager-master wazuh-manager-worker wazuh-manager; do
            if kubectl -n "$NS" get sts "$s" >/dev/null 2>&1; then
              kubectl -n "$NS" rollout restart sts/"$s"
              kubectl -n "$NS" rollout status  sts/"$s" --timeout=600s
            fi
          done
          # If running as a Deployment instead of STS:
          if kubectl -n "$NS" get deploy wazuh-manager >/dev/null 2>&1; then
            kubectl -n "$NS" rollout restart deploy/wazuh-manager
            kubectl -n "$NS" rollout status  deploy/wazuh-manager --timeout=600s
          fi

      - name: Verify passwordless (hard fail if misconfigured)
        shell: bash
        env:
          KUBECONFIG: ${{ env.KUBECONFIG }}
        run: |
          set -euo pipefail
          FAIL=0
          PODS="$(kubectl -n "$NS" get pods -l app=wazuh-manager -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' || true)"
          if [ -z "$PODS" ]; then
            echo "::error::No wazuh-manager pods found"
            exit 1
          fi
          for P in $PODS; do
            echo "Checking $P ..."
            RUNTIME_USEPW="$(kubectl -n "$NS" exec "$P" -c wazuh-manager -- sh -lc 'grep -o \"<use_password>[^<]*</use_password>\" /var/ossec/etc/ossec.conf || true')"
            PORT_OK="$(kubectl -n "$NS" exec "$P" -c wazuh-manager -- sh -lc 'grep -q \"<port>1515</port>\" /var/ossec/etc/ossec.conf && echo ok || echo bad')"
            AUTHD_PASS_PRESENT="$(kubectl -n "$NS" exec "$P" -c wazuh-manager -- sh -lc '[ -f /var/ossec/etc/authd.pass ] && echo present || echo absent')"
            echo "  use_password: ${RUNTIME_USEPW:-<none>}"
            echo "  port 1515 OK: $PORT_OK"
            echo "  authd.pass file: $AUTHD_PASS_PRESENT"
            if echo "${RUNTIME_USEPW}" | grep -q '<use_password>yes</use_password>'; then
              echo "::error file=/var/ossec/etc/ossec.conf,title=Password still enabled::$P has <use_password>yes</use_password>"
              FAIL=1
            fi
            if [ "$PORT_OK" != "ok" ]; then
              echo "::error file=/var/ossec/etc/ossec.conf,title=Wrong authd port::$P does not have <port>1515</port>"
              FAIL=1
            fi
            if [ "$AUTHD_PASS_PRESENT" = "present" ]; then
              echo "::error file=/var/ossec/etc/authd.pass,title=Password file present::$P has /var/ossec/etc/authd.pass present (should be absent)"
              FAIL=1
            fi
          done
          if [ "$FAIL" -ne 0 ]; then
            echo "::error::One or more wazuh-manager pods are not passwordless. See errors above."
            exit 1
          fi
          echo "All wazuh-manager pods verified passwordless on port 1515."

      - name: Final manager service check (non-destructive)
        shell: bash
        env:
          KUBECONFIG: ${{ env.KUBECONFIG }}
        run: |
          set -euo pipefail
          echo "Services in $NS (leaving existing 'wazuh' service untouched):"
          kubectl -n "$NS" get svc -o wide
          echo "DNS/port probe to existing service if present:"
          if kubectl -n "$NS" get svc wazuh >/dev/null 2>&1; then
            kubectl -n "$NS" delete pod probe-authd --ignore-not-found=true
            kubectl -n "$NS" run probe-authd --image=busybox:1.36 --restart=Never -- \
              sh -lc 'set -e; nslookup wazuh.wazuh.svc.cluster.local; for i in $(seq 1 12); do nc -vz -w 2 wazuh.wazuh.svc.cluster.local 1515 && exit 0; echo retry; sleep 2; done; exit 1'
            kubectl -n "$NS" logs pod/probe-authd || true
            kubectl -n "$NS" delete pod probe-authd --ignore-not-found=true
          else
            echo "::warning::Service 'wazuh' not found; your agent workflow should target whatever authd service you use."
          fi
