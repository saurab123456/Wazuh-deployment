name: Connectivity: GitHub â†” Ronin

on: { workflow_dispatch: {} }

jobs:
  probe:
    runs-on: [self-hosted, linux, X64, ronin, k8s, control-plane, vmware]
    env:
      API_HOST: "10.0.8.70"
      API_PORT: "6443"
      LOCAL_TUNNEL_PORT: "16443"
      KUBECONFIG: ${{ github.workspace }}/kubeconfig.yaml
    steps:
      - uses: actions/checkout@v4

      - name: Tools
        run: |
          set -euo pipefail
          if ! command -v kubectl >/dev/null 2>&1; then
            VER="$(curl -fsSL https://dl.k8s.io/release/stable.txt)"
            curl -fsSL -o kubectl "https://dl.k8s.io/release/${VER}/bin/linux/amd64/kubectl"
            chmod +x kubectl && sudo mv kubectl /usr/local/bin/
          fi
          sudo apt-get update -y >/dev/null 2>&1 || true
          sudo apt-get install -y --no-install-recommends openssh-client netcat-openbsd >/dev/null 2>&1 || true

      - name: Write kubeconfig
        run: |
          echo "${{ secrets.KUBE_CONFIG_B64 }}" | base64 -d > "$KUBECONFIG"
          chmod 600 "$KUBECONFIG"

      - name: Try direct API first
        id: direct
        continue-on-error: true
        run: |
          nc -zv -w2 "$API_HOST" "$API_PORT"
          curl -sk --connect-timeout 2 "https://${API_HOST}:${API_PORT}/healthz" || true

      - name: If direct OK, show cluster
        if: steps.direct.outcome == 'success'
        run: |
          kubectl cluster-info
          kubectl get nodes -o wide
          kubectl get ns

      # Fallback: SSH tunnel (needs BASTION_HOST/USER and SSH_PRIVATE_KEY secrets)
      - name: Add SSH key & known_hosts
        if: steps.direct.outcome != 'success'
        run: |
          for v in BASTION_HOST BASTION_USER SSH_PRIVATE_KEY; do
            test -n "${{ secrets[v] }}" || { echo "Missing secret: $v"; exit 1; }
          done
          mkdir -p ~/.ssh
          printf "%s" "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -T 10 "${{ secrets.BASTION_HOST }}" >> ~/.ssh/known_hosts || true

      - name: Start tunnel and patch kubeconfig
        if: steps.direct.outcome != 'success'
        run: |
          ssh -f -N -L "${LOCAL_TUNNEL_PORT}:${API_HOST}:${API_PORT}" \
            "${{ secrets.BASTION_USER }}@${{ secrets.BASTION_HOST }}" \
            -o ExitOnForwardFailure=yes -o ServerAliveInterval=30 -o ServerAliveCountMax=3
          # Point kubeconfig at the tunnel (skip hostname verify for this test)
          python3 - <<'PY'
import sys, yaml, os
p = os.environ["KUBECONFIG"]; port=os.environ["LOCAL_TUNNEL_PORT"]
cfg = yaml.safe_load(open(p))
for c in cfg["clusters"]:
    c["cluster"]["server"] = f"https://127.0.0.1:{port}"
    c["cluster"].pop("certificate-authority-data", None)
    c["cluster"]["insecure-skip-tls-verify"] = True
yaml.safe_dump(cfg, open(p,"w"))
PY

      - name: Show cluster via tunnel
        if: steps.direct.outcome != 'success'
        run: |
          kubectl cluster-info
          kubectl get nodes -o wide
          kubectl get ns

      - name: Cleanup tunnel
        if: always()
        run: |
          pkill -f "ssh -f -N -L ${LOCAL_TUNNEL_PORT}:${API_HOST}:${API_PORT}" || true
