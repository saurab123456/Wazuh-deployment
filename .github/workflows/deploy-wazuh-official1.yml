name: Deploy Wazuh (Official Kubernetes Manifests, passwordless)

on:
  workflow_dispatch:
    inputs:
      overlay:
        description: "Overlay to deploy (single or production) [ignored if repo uses single root kustomization]"
        required: true
        default: "single"
      wazuh_tag:
        description: "wazuh-kubernetes repo tag (e.g., v4.12.0)"
        required: true
        default: "v4.12.0"
      cleanup_unbound_pvcs:
        description: "Delete UNBOUND PVCs before deploy (true/false)"
        required: true
        default: "true"
      wipe_indexer_pvcs:
        description: "DANGER: delete ALL indexer PVCs if crashloop persists (true/false)"
        required: true
        default: "false"
      local_forward_port:
        description: "Local port for port-forward"
        required: true
        default: "8444"
      enable_port_forward:
        description: "Start port-forward automatically (true/false)"
        required: true
        default: "true"

jobs:
  deploy:
    runs-on:
      - self-hosted
      - linux
      - ronin
      - k8s
      - control-plane
    timeout-minutes: 180

    env:
      NS: wazuh
      KUBECONFIG: ${{ github.workspace }}/kubeconfig.yaml
      WAZUH_K8S_TAG: ${{ inputs.wazuh_tag }}
      OVERLAY_CHOICE: ${{ inputs.overlay }}
      CLEANUP_UNBOUND_PVCS: ${{ inputs.cleanup_unbound_pvcs }}
      WIPE_INDEXER_PVCS: ${{ inputs.wipe_indexer_pvcs }}
      LOCAL_FORWARD_PORT: ${{ inputs.local_forward_port }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Ensure kubectl
        shell: bash
        run: |
          set -e
          if ! command -v kubectl >/dev/null 2>&1; then
            KVER="$(curl -sS https://dl.k8s.io/release/stable.txt)"
            curl -sSLo kubectl "https://dl.k8s.io/release/${KVER}/bin/linux/amd64/kubectl"
            chmod +x kubectl && sudo mv kubectl /usr/local/bin/
          fi
          kubectl version --client=true | head -n 1

      - name: Install yq (YAML CLI)
        shell: bash
        run: |
          set -e
          if ! command -v yq >/dev/null 2>&1; then
            sudo curl -sSL -o /usr/local/bin/yq https://github.com/mikefarah/yq/releases/download/v4.44.3/yq_linux_amd64
            sudo chmod +x /usr/local/bin/yq
          fi
          yq --version

      - name: Install jq
        shell: bash
        run: |
          set -e
          if ! command -v jq >/dev/null 2>&1; then
            sudo apt-get update -y
            sudo apt-get install -y jq
          fi
          jq --version

      - name: Write kubeconfig from secret
        shell: bash
        env:
          KUBE_CONFIG_B64: ${{ secrets.KUBE_CONFIG_B64 }}
        run: |
          set -e
          test -n "$KUBE_CONFIG_B64" || { echo "::error::Secret KUBE_CONFIG_B64 missing"; exit 1; }
          echo "$KUBE_CONFIG_B64" | base64 -d > "$KUBECONFIG"
          kubectl cluster-info
          kubectl get nodes -o wide

      - name: Ensure namespace
        shell: bash
        run: |
          kubectl create namespace "$NS" --dry-run=client -o yaml | kubectl apply -f -

      # ---------- keep vm.max_map_count enforced & verifiable ----------
      - name: Ensure vm.max_map_count on all nodes (DaemonSet kept)
        shell: bash
        run: |
          cat > /tmp/wazuh-sysctl-ds.yaml <<'EOF'
          apiVersion: apps/v1
          kind: DaemonSet
          metadata: { name: wazuh-sysctl, namespace: kube-system, labels: { app: wazuh-sysctl } }
          spec:
            selector: { matchLabels: { app: wazuh-sysctl } }
            template:
              metadata: { labels: { app: wazuh-sysctl } }
              spec:
                hostPID: true
                tolerations: [ { operator: "Exists" } ]
                containers:
                  - name: sysctl
                    image: busybox:1.36
                    securityContext: { privileged: true }
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        set -e
                        echo 262144 > /host-proc/sys/vm/max_map_count
                        mkdir -p /host-etc/sysctl.d
                        echo "vm.max_map_count=262144" > /host-etc/sysctl.d/99-wazuh.conf
                        echo "node: $(hostname) vm.max_map_count=$(cat /host-proc/sys/vm/max_map_count)"
                        sleep 3600
                    volumeMounts:
                      - { name: host-proc, mountPath: /host-proc }
                      - { name: host-etc,  mountPath: /host-etc  }
                volumes:
                  - { name: host-proc, hostPath: { path: /proc, type: Directory } }
                  - { name: host-etc,  hostPath: { path: /etc,  type: Directory } }
          EOF
          kubectl apply -f /tmp/wazuh-sysctl-ds.yaml
          sleep 5
          kubectl -n kube-system get pods -l app=wazuh-sysctl -o wide || true
          for p in $(kubectl -n kube-system get pods -l app=wazuh-sysctl -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'); do
            kubectl -n kube-system logs "$p" || true
          done

      # ---------- dynamic StorageClass alias ----------
      - name: Ensure StorageClass alias 'wazuh-storage'
        shell: bash
        run: |
          set -euo pipefail

          pick_dynamic_sc() {
            local def dyn
            def="$(kubectl get sc -o json \
                | yq -r '.items[]
                         | select(.provisioner != "kubernetes.io/no-provisioner")
                         | select(.metadata.annotations."storageclass.kubernetes.io/is-default-class" == "true")
                         | .metadata.name' | head -n1 || true)"
            if [ -n "${def:-}" ]; then
              echo "$def"; return 0
            fi
            dyn="$(kubectl get sc -o json \
                | yq -r '.items[]
                         | select(.provisioner != "kubernetes.io/no-provisioner")
                         | .metadata.name' | head -n1 || true)"
            [ -n "${dyn:-}" ] && echo "$dyn"
          }

          TARGET_SC="$(pick_dynamic_sc || true)"

          if [ -z "${TARGET_SC:-}" ]; then
            echo "No dynamic StorageClass found. Installing local-path-provisioner …"
            kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
            kubectl -n local-path-storage rollout status deploy/local-path-provisioner --timeout=180s
            kubectl annotate sc local-path storageclass.kubernetes.io/is-default-class="true" --overwrite || true
            TARGET_SC="local-path"
          fi

          echo "Using dynamic StorageClass: ${TARGET_SC}"

          PROV="$(kubectl get sc "$TARGET_SC" -o jsonpath='{.provisioner}')"

          cat <<EOF | kubectl apply -f -
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: wazuh-storage
          provisioner: ${PROV}
          reclaimPolicy: Delete
          volumeBindingMode: WaitForFirstConsumer
          allowVolumeExpansion: true
          EOF

          kubectl get sc -o wide

      - name: Fetch official wazuh-kubernetes repo
        shell: bash
        run: |
          set -e
          rm -rf wazuh-kubernetes
          git clone --depth 1 --branch "$WAZUH_K8S_TAG" https://github.com/wazuh/wazuh-kubernetes.git

      - name: Resolve overlay path (auto-detect)
        id: resolve
        shell: bash
        run: |
          set -euo pipefail
          ROOT="wazuh-kubernetes/wazuh"
          if [ -f "$ROOT/kustomization.yml" ] || [ -f "$ROOT/kustomization.yaml" ]; then
            PICK="$ROOT"
          else
            PICK="$(find "$ROOT" -type f \( -name 'kustomization.yml' -o -name 'kustomization.yaml' \) -printf '%h\n' | sort -u | head -n1 || true)"
          fi
          test -n "${PICK:-}" || { echo "::error::Could not find any kustomize overlays under $ROOT"; exit 1; }
          echo "overlay_path=$PICK" >> "$GITHUB_OUTPUT"

      - name: Generate self-signed certs for Kustomize (indexer + dashboard_http)
        shell: bash
        run: |
          set -e
          CERTROOT="${{ github.workspace }}/wazuh-kubernetes/wazuh/certs"
          IC="${CERTROOT}/indexer_cluster"
          DH="${CERTROOT}/dashboard_http"
          mkdir -p "$IC" "$DH"
          cat > /tmp/wazuh-openssl.cnf <<'CONF'
          [req]
          default_bits=2048
          prompt=no
          default_md=sha256
          req_extensions = v3_req
          distinguished_name=req_dn
          [req_dn]
          CN=wazuh-local
          [v3_req]
          basicConstraints = CA:FALSE
          keyUsage = digitalSignature, keyEncipherment
          extendedKeyUsage = serverAuth, clientAuth
          subjectAltName = @alt_names
          [alt_names]
          DNS.1 = wazuh-indexer
          DNS.2 = wazuh-indexer.wazuh.svc
          DNS.3 = wazuh-dashboard
          DNS.4 = wazuh-dashboard.wazuh.svc
          DNS.5 = localhost
          IP.1  = 127.0.0.1
          CONF
          openssl genrsa -out "$IC/root-ca-key.pem" 4096
          openssl req -x509 -new -key "$IC/root-ca-key.pem" -sha256 -days 3650 -out "$IC/root-ca.pem" -subj "/CN=Wazuh Root CA"
          sign_ic() {
            local name="$1"
            openssl genrsa -out "$IC/${name}-key.pem" 2048
            openssl req -new -key "$IC/${name}-key.pem" -out "$IC/${name}.csr" -config /tmp/wazuh-openssl.cnf
            openssl x509 -req -in "$IC/${name}.csr" -CA "$IC/root-ca.pem" -CAkey "$IC/root-ca-key.pem" -CAcreateserial -out "$IC/${name}.pem" -days 825 -sha256 -extensions v3_req -extfile /tmp/wazuh-openssl.cnf
            rm -f "$IC/${name}.csr"
          }
          for n in admin dashboard node filebeat; do sign_ic "$n"; done
          openssl genrsa -out "$DH/key.pem" 2048
          openssl req -new -key "$DH/key.pem" -out "$DH/dashboard.csr" -config /tmp/wazuh-openssl.cnf
          openssl x509 -req -in "$DH/dashboard.csr" -CA "$IC/root-ca.pem" -CAkey "$IC/root-ca-key.pem" -CAcreateserial -out "$DH/cert.pem" -days 825 -sha256 -extensions v3_req -extfile /tmp/wazuh-openssl.cnf
          rm -f "$DH/dashboard.csr"

      - name: Build overlay with Kustomize
        shell: bash
        run: |
          set -e
          kubectl kustomize "${{ steps.resolve.outputs.overlay_path }}" > /tmp/wazuh-all.yaml
          kubectl -n "$NS" delete deploy wazuh-dashboard --ignore-not-found=true

      # ---------- Make bigger default PVC sizes before any apply ----------
      - name: Bump default PVC sizes in manifest (before apply)
        shell: bash
        run: |
          set -euo pipefail
          F=/tmp/wazuh-all.yaml
          # Indexer PVCs -> 60Gi
          yq -i '
            (. | select(.kind=="StatefulSet" and .metadata.name=="wazuh-indexer")
               .spec.volumeClaimTemplates[]? // {} )
            |= (.spec.resources.requests.storage = "60Gi")
          ' "$F"
          # Manager master PVCs -> 20Gi
          yq -i '
            (. | select(.kind=="StatefulSet" and .metadata.name=="wazuh-manager-master")
               .spec.volumeClaimTemplates[]? // {} )
            |= (.spec.resources.requests.storage = "20Gi")
          ' "$F"

      # --- ONLY touch ConfigMaps that actually contain data.ossec.conf ---
      - name: Enforce passwordless <auth> in manager ConfigMaps (pre-apply)
        shell: bash
        run: |
          set -euo pipefail
          FILE=/tmp/wazuh-all.yaml
          yq -i '
            (. | select(.kind=="ConfigMap" and has("data") and .data."ossec.conf")) |=
            (.data."ossec.conf" =
              (.data."ossec.conf"
                | sub("(?s)(<auth>)(.*?)(</auth>)";
                      capture("(?s)(?<open><auth>)(?<body>.*?)(?<close></auth>)") as $m
                      | ($m.body
                          | sub("<use_password>[^<]+</use_password>"; "<use_password>no</use_password>")
                          | sub("<port>[^<]+</port>"; "<port>1515</port>")
                          | sub("(?s)[[:space:]]*<password>[^<]*</password>[[:space:]]*"; "")
                        ) as $nb
                      | "\($m.open)\($nb)\($m.close)"
                )
              )
            )
          ' "$FILE"

      - name: Remove auth password Secret mounts (passwordless)
        shell: bash
        run: |
          set -euo pipefail
          IN=/tmp/wazuh-all.yaml
          OUT=/tmp/wazuh-all-passwordless.yaml
          yq eval -o=y '
            (.. | select(tag=="!!map" and has("kind") and .kind=="StatefulSet"))
            |= (
              .spec.template.spec.volumes
                = (.spec.template.spec.volumes // [] | map(select(.name!="wazuh-authd-pass" and .name!="wazuh-enroll-pass"))) |
              .spec.template.spec.containers
                = (.spec.template.spec.containers // [] | map(
                    .volumeMounts = (.volumeMounts // [] | map(select(.name!="wazuh-authd-pass" and .name!="wazuh-enroll-pass"))) |
                    .env = (.env // [] | map(select(.name!="WAZUH_MANAGER_PASSWORD" and .name!="WAZUH_AUTHD_PASSWORD")))
                  ))
            )
          ' "$IN" > "$OUT"
          mv "$OUT" /tmp/wazuh-all.yaml

      - name: Optional cleanup (delete UNBOUND PVCs only)
        shell: bash
        run: |
          set -e
          if [ "${CLEANUP_UNBOUND_PVCS}" = "true" ]; then
            UNBOUND="$(kubectl -n "$NS" get pvc --no-headers 2>/dev/null | awk '$2!="Bound"{print $1}')"
            if [ -n "$UNBOUND" ]; then
              for p in $UNBOUND; do kubectl -n "$NS" delete pvc "$p" --ignore-not-found; done
            fi
          fi

      - name: "OPTIONAL: wipe ALL indexer PVCs (if set)"
        if: ${{ inputs.wipe_indexer_pvcs == 'true' }}
        shell: bash
        run: |
          set -e
          echo "::warning::Deleting ALL indexer PVCs (data loss for indexer)."
          for pvc in $(kubectl -n "$NS" get pvc -o name | grep -E 'data-wazuh-indexer-[0-9]+$' || true); do
            kubectl -n "$NS" delete "$pvc" --ignore-not-found=true
          done

      - name: Apply non-StatefulSet resources (filter out StorageClass)
        shell: bash
        run: |
          set -e
          yq -o=y e 'select(.kind != "StorageClass" and .kind != "StatefulSet")' /tmp/wazuh-all.yaml > /tmp/wazuh-no-sts-sc.yaml
          if [ -s /tmp/wazuh-no-sts-sc.yaml ]; then kubectl -n "$NS" apply -f /tmp/wazuh-no-sts-sc.yaml; fi

      - name: Recreate StatefulSets to avoid immutability errors
        shell: bash
        run: |
          set -e
          NAMES="$(yq -r 'select(.kind == "StatefulSet" and .metadata.name != null) | .metadata.name' /tmp/wazuh-all.yaml | sed 's/\r$//' | grep -E '^[a-z0-9]([-a-z0-9]*[a-z0-9])?$' || true)"
          if [ -n "$NAMES" ]; then
            for n in $NAMES; do kubectl -n "$NS" delete statefulset "$n" --ignore-not-found=true; done
            yq -o=y e 'select(.kind == "StatefulSet")' /tmp/wazuh-all.yaml > /tmp/wazuh-sts.yaml
            kubectl -n "$NS" apply -f /tmp/wazuh-sts.yaml
          fi

      - name: Add initContainer guard to managers (force passwordless on boot)
        shell: bash
        run: |
          set -euo pipefail
          for sts in wazuh-manager-master wazuh-manager-worker; do
            kubectl -n "$NS" get sts "$sts" >/dev/null 2>&1 || continue
            kubectl -n "$NS" patch statefulset "$sts" --type='merge' -p '
            spec:
              template:
                spec:
                  initContainers:
                  - name: force-passwordless-authd
                    image: busybox:1.36
                    securityContext:
                      runAsUser: 0
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        set -e
                        f=/var/ossec/etc/ossec.conf
                        [ -f "$f" ] || exit 0
                        awk '"'"'
                          BEGIN{in=0}
                          /<auth>/{in=1}
                          in && /<password>.*<\/password>/{next}
                          /<\/auth>/{in=0}
                          {print}
                        '"'"' "$f" > "$f.tmp" && mv "$f.tmp" "$f" || true
                        sed -i -E "/<auth>/,/<\\/auth>/ s|<use_password>[^<]+</use_password>|<use_password>no</use_password>|" "$f" || true
                        sed -i -E "/<auth>/,/<\\/auth>/ s|<port>[^<]+</port>|<port>1515</port>|" "$f" || true
                        rm -f /var/ossec/etc/authd.pass || true
            ' || true
          done

      # -------------------- TUNE: requests-only + JVM + spreading --------------------
      - name: Tune indexers for capacity (2 CPU / 8Gi each, bigger heap)
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n "$NS" patch sts wazuh-indexer --type=merge -p '{
            "spec": {
              "template": {
                "metadata": { "labels": { "app": "wazuh-indexer" } },
                "spec": {
                  "securityContext": { "runAsUser": 1000, "runAsGroup": 1000, "fsGroup": 1000 },
                  "containers": [ { "name": "wazuh-indexer",
                    "env": [ { "name": "OPENSEARCH_JAVA_OPTS", "value": "-Xms4g -Xmx4g -XX:MaxDirectMemorySize=1g -Dlog4j2.formatMsgNoLookups=true" } ],
                    "resources": { "requests": { "cpu": "2", "memory": "8Gi" } }
                  } ],
                  "affinity": {
                    "podAntiAffinity": {
                      "preferredDuringSchedulingIgnoredDuringExecution": [ { "weight": 100, "podAffinityTerm": {
                        "labelSelector": { "matchLabels": { "app": "wazuh-indexer" } },
                        "topologyKey": "kubernetes.io/hostname"
                      } } ]
                    }
                  },
                  "topologySpreadConstraints": [ { "maxSkew": 1, "topologyKey": "kubernetes.io/hostname",
                    "whenUnsatisfiable": "ScheduleAnyway", "labelSelector": { "matchLabels": { "app": "wazuh-indexer" } } } ]
                }
              }
            }
          }'
          kubectl -n "$NS" patch statefulset wazuh-indexer --type='json' -p='[
            {"op":"remove","path":"/spec/template/spec/containers/0/resources/limits"}
          ]' 2>/dev/null || true

      - name: Tune managers & dashboard (bigger requests, no limits)
        shell: bash
        run: |
          set -euo pipefail

          set_resources_clear_limits() {
            local kind="$1" name="$2" container="$3" cpu="$4" mem="$5"
            kubectl -n "$NS" get "$kind" "$name" >/dev/null 2>&1 || return 0
            kubectl -n "$NS" set resources "$kind/$name" \
              --containers="$container" --requests="cpu=$cpu,memory=$mem" --limits="" || true
            idx="$(kubectl -n "$NS" get "$kind" "$name" -o json | jq -r \
              --arg c "$container" '.spec.template.spec.containers
                | to_entries | map(select(.value.name==$c)) | .[0].key // empty')"
            [ -n "$idx" ] && kubectl -n "$NS" patch "$kind" "$name" --type=json \
              -p="[ {\"op\":\"remove\",\"path\":\"/spec/template/spec/containers/$idx/resources/limits\"} ]" 2>/dev/null || true
          }

          # Master 1 CPU / 4Gi
          set_resources_clear_limits statefulset wazuh-manager-master wazuh-manager 1 4Gi
          # Workers 1 CPU / 3Gi (supports single-STS or numbered)
          if kubectl -n "$NS" get sts wazuh-manager-worker >/dev/null 2>&1; then
            set_resources_clear_limits statefulset wazuh-manager-worker wazuh-manager 1 3Gi
          else
            for st in wazuh-manager-worker-0 wazuh-manager-worker-1; do
              set_resources_clear_limits statefulset "$st" wazuh-manager 1 3Gi
            done
          fi

          # Prefer managers on different nodes
          for sts in wazuh-manager-master wazuh-manager-worker wazuh-manager-worker-0 wazuh-manager-worker-1; do
            kubectl -n "$NS" get sts "$sts" >/dev/null 2>&1 || continue
            kubectl -n "$NS" patch sts "$sts" --type=merge -p '{
              "spec":{"template":{"spec":{
                "affinity":{"podAntiAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":[{
                  "weight":100,
                  "podAffinityTerm":{
                    "labelSelector":{"matchLabels":{"app":"wazuh-manager"}},
                    "topologyKey":"kubernetes.io/hostname"
                  }}]}}
              }}}}'
          done

          # Dashboard 500m / 2Gi
          set_resources_clear_limits deployment wazuh-dashboard wazuh-dashboard 500m 2Gi

      - name: Resize PVCs (Indexer→60Gi, Manager master→20Gi, Dashboard→10Gi)
        shell: bash
        run: |
          set -euo pipefail
          supports_expansion () {
            local pvc="$1" sc
            sc="$(kubectl -n "$NS" get pvc "$pvc" -o jsonpath='{.spec.storageClassName}' 2>/dev/null || true)"
            [ -z "$sc" ] && { echo "no"; return; }
            kubectl get sc "$sc" -o jsonpath='{.allowVolumeExpansion}' 2>/dev/null | grep -qi true && echo "yes" || echo "no"
          }
          resize_pvc () { # name size
            kubectl -n "$NS" get pvc "$1" >/dev/null 2>&1 || { echo "::warning::PVC $1 not found"; return; }
            if [ "$(supports_expansion "$1")" != "yes" ]; then
              echo "::error::StorageClass for PVC $1 does not allow expansion. (Use wipe_indexer_pvcs=true for indexers.)"; return
            fi
            echo "Resizing $1 -> $2"
            kubectl -n "$NS" patch pvc "$1" --type=merge -p "{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"$2\"}}}}"
          }
          for i in 0 1 2; do resize_pvc "wazuh-indexer-wazuh-indexer-$i" "60Gi" || true; done
          if kubectl -n "$NS" get pod wazuh-manager-master-0 >/dev/null 2>&1; then
            mapfile -t MM < <(kubectl -n "$NS" get pod wazuh-manager-master-0 -o json \
              | jq -r '.spec.volumes[] | select(.persistentVolumeClaim) | .persistentVolumeClaim.claimName')
            for pvc in "${MM[@]:-}"; do resize_pvc "$pvc" "20Gi"; done
          fi
          DPOD="$(kubectl -n "$NS" get pods -l app=wazuh-dashboard -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
          if [ -n "${DPOD:-}" ]; then
            mapfile -t DPVCS < <(kubectl -n "$NS" get pod "$DPOD" -o json \
              | jq -r '.spec.volumes[] | select(.persistentVolumeClaim) | .persistentVolumeClaim.claimName')
            for pvc in "${DPVCS[@]:-}"; do resize_pvc "$pvc" "10Gi"; done
          fi
          kubectl -n "$NS" get pvc

      - name: Wait & diagnose core components
        shell: bash
        run: |
          set -e
          for sel in "app=wazuh-indexer" "app=wazuh-manager" "app=wazuh-dashboard"; do
            echo "Waiting for: $sel"
            kubectl -n "$NS" wait --for=condition=Ready pod -l "$sel" --timeout=1200s || true
            kubectl -n "$NS" get pods -l "$sel" -o wide
          done

      - name: Patch manager ossec.conf in live ConfigMaps (passwordless)
        shell: bash
        run: |
          set -euo pipefail
          NS="${NS:-wazuh}"
          CM_NAMES="$(
            { kubectl -n "$NS" get pods -l app=wazuh-manager \
                -o jsonpath='{range .items[*].spec.volumes[*]}{.configMap.name}{"\n"}{end}';
              kubectl -n "$NS" get sts -l app=wazuh-manager \
                -o jsonpath='{range .items[*].spec.template.spec.volumes[*]}{.configMap.name}{"\n"}{end}';
            } 2>/dev/null | sort -u | sed '/^$/d' || true
          )"
          test -n "$CM_NAMES" || { echo "::warning::No manager ConfigMaps found"; exit 0; }
          for CM in $CM_NAMES; do
            echo ">> Processing ConfigMap: $CM"
            kubectl -n "$NS" get cm "$CM" -o yaml > "/tmp/${CM}.yaml"
            yq -r '.data."ossec.conf"' "/tmp/${CM}.yaml" > /tmp/ossec.conf.cm || true
            if [ ! -s /tmp/ossec.conf.cm ] || grep -qx 'null' /tmp/ossec.conf.cm; then
              echo "::warning::${CM} has no data.ossec.conf; skipping"
              continue
            fi
            sed -i -E '/<auth>/,/<\/auth>/ s|<use_password>[^<]+</use_password>|<use_password>no</use_password>|' /tmp/ossec.conf.cm
            sed -i -E '/<auth>/,/<\/auth>/ s|<port>[0-9]+</port>|<port>1515</port>|' /tmp/ossec.conf.cm
            awk 'BEGIN{inauth=0} /<auth>/{inauth=1} inauth && /<password>.*<\/password>/{next} /<\/auth>/{inauth=0} {print}' /tmp/ossec.conf.cm > /tmp/ossec.conf.cm.new && mv /tmp/ossec.conf.cm.new /tmp/ossec.conf.cm
            yq -i '.data."ossec.conf" = load_str("/tmp/ossec.conf.cm")' "/tmp/${CM}.yaml"
            kubectl -n "$NS" apply -f "/tmp/${CM}.yaml"
          done

      - name: Force passwordless on ALL managers (in-pod sed + purge authd.pass)
        shell: bash
        run: |
          set -euo pipefail
          export NS="${NS:-wazuh}"
          for P in $(kubectl -n "$NS" get pods -l app=wazuh-manager -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'); do
            echo "Patching $P ..."
            kubectl -n "$NS" exec "$P" -c wazuh-manager -- sh -lc '
              set -e
              f=/var/ossec/etc/ossec.conf
              cp "$f" "${f}.bak.$(date +%s)" || true
              sed -i -E "/<auth>/,/<\\/auth>/ s|<port>[0-9]+</port>|<port>1515</port>|" "$f"
              sed -i -E "/<auth>/,/<\\/auth>/ s|<use_password>[^<]+</use_password>|<use_password>no</use_password>|" "$f"
              awk '"'"'BEGIN{inauth=0} /<auth>/{inauth=1} inauth && /<password>.*<\/password>/{next} /<\/auth>/{inauth=0} {print}'"'"' "$f" > "$f.tmp" && mv "$f.tmp" "$f"
              rm -f /var/ossec/etc/authd.pass || true
              echo "----- <auth> after patch (pod $(hostname)) -----"
              sed -n "/<auth>/,/<\\/auth>/p" "$f"
            '
          done

      - name: Rollout restart manager StatefulSets (apply passwordless from CM)
        shell: bash
        run: |
          set -e
          for s in wazuh-manager-master wazuh-manager-worker; do
            kubectl -n "$NS" get sts "$s" >/dev/null 2>&1 || continue
            kubectl -n "$NS" rollout restart sts/"$s"
            kubectl -n "$NS" rollout status  sts/"$s" --timeout=600s
          done
          echo "Verify <auth> blocks in running pods:"
          for P in $(kubectl -n "$NS" get pods -l app=wazuh-manager -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'); do
            echo "=== $P ==="
            kubectl -n "$NS" exec "$P" -c wazuh-manager -- sh -lc 'sed -n "/<auth>/,/<\\/auth>/p" /var/ossec/etc/ossec.conf'
          done

      - name: Verify managers are passwordless (hard fail if misconfigured)
        shell: bash
        run: |
          set -euo pipefail
          FAIL=0
          PODS="$(kubectl -n "$NS" get pods -l app=wazuh-manager -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' || true)"
          if [ -z "$PODS" ]; then
            echo "::error::No wazuh-manager pods found"
            exit 1
          fi
          for P in $PODS; do
            echo "Checking $P ..."
            RUNTIME_LINE="$(kubectl -n "$NS" exec "$P" -c wazuh-manager -- sh -lc 'grep -o \"<use_password>[^<]*</use_password>\" /var/ossec/etc/ossec.conf || true')"
            MOUNTED_LINE="$(kubectl -n "$NS" exec "$P" -c wazuh-manager -- sh -lc '[ -f /wazuh-config-mount/etc/ossec.conf ] && grep -o \"<use_password>[^<]*</use_password>\" /wazuh-config-mount/etc/ossec.conf || true')"
            PORT_OK="$(kubectl -n "$NS" exec "$P" -c wazuh-manager -- sh -lc 'grep -q \"<port>1515</port>\" /var/ossec/etc/ossec.conf && echo ok || echo bad')"
            AUTHD_PASS_PRESENT="$(kubectl -n "$NS" exec "$P" -c wazuh-manager -- sh -lc '[ -f /var/ossec/etc/authd.pass ] && echo present || echo absent')"
            echo "  runtime use_password: ${RUNTIME_LINE:-<none>}"
            echo "  mounted use_password: ${MOUNTED_LINE:-<none or no mount>}"
            echo "  runtime port 1515: $PORT_OK"
            echo "  authd.pass: $AUTHD_PASS_PRESENT"
            if echo "${RUNTIME_LINE}" | grep -q '<use_password>yes</use_password>'; then
              echo "::error file=/var/ossec/etc/ossec.conf,title=Runtime shows password enabled::$P has <use_password>yes</use_password>"; FAIL=1; fi
            if [ -n "${MOUNTED_LINE}" ] && echo "${MOUNTED_LINE}" | grep -q '<use_password>yes</use_password>'; then
              echo "::error file=/wazuh-config-mount/etc/ossec.conf,title=Mounted CM shows password enabled::$P has <use_password>yes</use_password> in ConfigMap"; FAIL=1; fi
            if [ "$PORT_OK" != "ok" ]; then
              echo "::error file=/var/ossec/etc/ossec.conf,title=Wrong authd port::$P does not have <port>1515</port>"; FAIL=1; fi
            if [ "$AUTHD_PASS_PRESENT" = "present" ]; then
              echo "::error file=/var/ossec/etc/authd.pass,title=Password file present::$P has /var/ossec/etc/authd.pass present (should be absent)"; FAIL=1; fi
          done
          if [ "$FAIL" -ne 0 ]; then
            echo "::error::One or more wazuh-manager pods are not passwordless. See errors above."
            exit 1
          fi
          echo "All wazuh-manager pods verified passwordless and on port 1515."

      - name: Ensure Services (master-only 1515 + workers 1514)
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n "$NS" delete svc wazuh --ignore-not-found=true
          cat <<'EOF' | kubectl -n "$NS" apply -f -
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: wazuh-authd-master
            labels: { app: wazuh-manager, node-type: master }
          spec:
            type: ClusterIP
            selector: { app: wazuh-manager, node-type: master }
            ports:
              - { name: authd, port: 1515, targetPort: 1515, protocol: TCP }
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: wazuh-workers
            labels: { app: wazuh-manager, role: agents-events }
          spec:
            type: ClusterIP
            selector: { app: wazuh-manager, node-type: worker }
            ports:
              - { name: agents-events, port: 1514, targetPort: 1514, protocol: TCP }
          EOF
          kubectl -n "$NS" get svc -o wide
          kubectl -n "$NS" get endpoints -o wide

      - name: Create Agent ConfigMap (passwordless enrollment)
        shell: bash
        run: |
          set -euo pipefail
          cat > /tmp/agent-ossec.conf <<'EOF'
          <ossec_config>
            <client>
              <server>
                <address>wazuh-workers.wazuh.svc.cluster.local</address>
                <port>1514</port>
                <protocol>tcp</protocol>
              </server>
              <enrollment>
                <enabled>yes</enabled>
                <manager_address>wazuh-authd-master.wazuh.svc.cluster.local</manager_address>
                <port>1515</port>
                <use_password>no</use_password>
                <ssl_verify_host>no</ssl_verify_host>
                <auto_negotiate>no</auto_negotiate>
              </enrollment>
            </client>
          </ossec_config>
          EOF
          kubectl -n "$NS" create configmap wazuh-agent-ossec --from-file=ossec.conf=/tmp/agent-ossec.conf -o yaml --dry-run=client | kubectl apply -f -

      - name: Deploy Agent DaemonSet (requests-only, no limits)
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n "$NS" delete ds wazuh-agent --ignore-not-found=true
          cat <<'EOF' | kubectl -n "$NS" apply -f -
          apiVersion: apps/v1
          kind: DaemonSet
          metadata:
            name: wazuh-agent
            labels: { app: wazuh-agent }
          spec:
            selector: { matchLabels: { app: wazuh-agent } }
            updateStrategy: { type: RollingUpdate }
            template:
              metadata: { labels: { app: wazuh-agent } }
              spec:
                hostNetwork: true
                hostPID: true
                dnsPolicy: ClusterFirstWithHostNet
                terminationGracePeriodSeconds: 30
                tolerations:
                  - { key: node-role.kubernetes.io/master, operator: Exists, effect: NoSchedule }
                  - { key: node-role.kubernetes.io/control-plane, operator: Exists, effect: NoSchedule }
                containers:
                  - name: wazuh-agent
                    image: ghcr.io/saurab123456/wazuh-agent:4.12.0
                    imagePullPolicy: IfNotPresent
                    securityContext: { privileged: true, runAsUser: 0 }
                    env:
                      - name: NODE_NAME
                        valueFrom: { fieldRef: { fieldPath: spec.nodeName } }
                      - name: AUTHD_PORT
                        value: "1515"
                      - name: MANAGER_DNS
                        value: "wazuh-authd-master.wazuh.svc.cluster.local"
                      - name: GROUPS
                        value: "kubernetes"
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        set -e
                        AGENT_NAME="${NODE_NAME}"
                        echo "Agent name: $AGENT_NAME"
                        mkdir -p /var/ossec/var/run /var/ossec/queue/sockets /var/ossec/queue/db
                        cp /config/ossec.conf /var/ossec/etc/ossec.conf 2>/dev/null || true
                        rm -f /var/ossec/var/run/*.pid /var/ossec/var/*.lock 2>/dev/null || true
                        enroll() {
                          echo "Enrolling to ${MANAGER_DNS}:${AUTHD_PORT} (group=${GROUPS})…"
                          if [ -n "${GROUPS}" ]; then
                            /var/ossec/bin/agent-auth -m "${MANAGER_DNS}" -p "${AUTHD_PORT}" -A "${AGENT_NAME}" -G "${GROUPS}"
                          else
                            /var/ossec/bin/agent-auth -m "${MANAGER_DNS}" -p "${AUTHD_PORT}" -A "${AGENT_NAME}"
                          fi
                        }
                        if [ ! -s /var/ossec/etc/client.keys ]; then
                          for i in $(seq 1 36); do
                            if enroll; then break; fi
                            echo "Enroll attempt $i failed; retrying in 5s…"
                            sleep 5
                          done
                        fi
                        if [ ! -s /var/ossec/etc/client.keys ]; then
                          echo "ERROR: Enrollment failed; exiting."
                          exit 1
                        fi
                        echo "Starting Wazuh agent…"
                        /var/ossec/bin/wazuh-control start
                        sleep 5
                        /var/ossec/bin/wazuh-control status
                        tail -f /var/ossec/logs/ossec.log
                    readinessProbe:
                      exec: { command: ["/bin/sh","-c",'/var/ossec/bin/wazuh-control status | grep -q "is running"'] }
                      initialDelaySeconds: 30
                      periodSeconds: 10
                      failureThreshold: 6
                      timeoutSeconds: 5
                    livenessProbe:
                      exec: { command: ["/bin/sh","-c",'/var/ossec/bin/wazuh-control status | grep -q "is running"'] }
                      initialDelaySeconds: 60
                      periodSeconds: 30
                      failureThreshold: 3
                      timeoutSeconds: 5
                    lifecycle:
                      preStop:
                        exec: { command: ["/bin/sh","-c","/var/ossec/bin/wazuh-control stop || true"] }
                    resources:
                      requests: { cpu: "100m", memory: "256Mi" }
                    volumeMounts:
                      - { name: ossec-conf-cm,   mountPath: /config,                    readOnly: true }
                      - { name: varlogcontainers, mountPath: /host/var/log/containers,   readOnly: true }
                      - { name: varlogpods,       mountPath: /host/var/log/pods,         readOnly: true }
                      - { name: varlog,           mountPath: /host/var/log,              readOnly: true }
                volumes:
                  - { name: ossec-conf-cm,   configMap: { name: wazuh-agent-ossec } }
                  - { name: varlogcontainers, hostPath: { path: /var/log/containers, type: DirectoryOrCreate } }
                  - { name: varlogpods,       hostPath: { path: /var/log/pods,       type: DirectoryOrCreate } }
                  - { name: varlog,           hostPath: { path: /var/log,            type: Directory } }
          EOF
          kubectl -n "$NS" rollout status ds/wazuh-agent --timeout=600s || true
          kubectl -n "$NS" get pods -l app=wazuh-agent -o wide

      - name: Probe authd (1515) via master-only Service
        shell: bash
        run: |
          set -euo pipefail
          echo "Probing TCP connectivity to wazuh-authd-master:1515 …"
          kubectl -n "$NS" delete pod probe-authd --ignore-not-found=true
          kubectl -n "$NS" run probe-authd --image=busybox:1.36 --restart=Never -- \
            sh -lc 'set -e; nslookup wazuh-authd-master || true; for i in $(seq 1 12); do nc -vz -w 2 wazuh-authd-master 1515 && exit 0; echo retry; sleep 2; done; exit 1'
          kubectl -n "$NS" logs pod/probe-authd || true
          kubectl -n "$NS" delete pod probe-authd --ignore-not-found=true

      - name: Discover Dashboard port
        id: dash
        shell: bash
        run: |
          set -euo pipefail
          PORT="$(kubectl -n "$NS" get svc -l app=wazuh-dashboard -o jsonpath='{.items[0].spec.ports[0].port}' 2>/dev/null || true)"
          if [ -z "${PORT:-}" ]; then
            PORT="$(kubectl -n "$NS" get deploy wazuh-dashboard -o jsonpath='{.spec.template.spec.containers[0].ports[0].containerPort}' 2>/dev/null || true)"
          fi
          test -n "${PORT:-}" || { echo "::error::Could not determine dashboard port"; exit 1; }
          echo "dash_port=${PORT}" >> "$GITHUB_OUTPUT"

      - name: Indexer quick logs (if crashloop)
        shell: bash
        run: |
          set -e
          pods=$(kubectl -n "$NS" get pods -l app=wazuh-indexer -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.containerStatuses[0].state.waiting.reason}{"\n"}{end}' || true)
          echo "$pods" | grep -qi crash && {
            for p in $(echo "$pods" | awk '$2 ~ /CrashLoopBackOff/ {print $1}'); do
              echo "==== Logs: $p ===="
              kubectl -n "$NS" logs "$p" --tail=200 || true
              echo "==== Describe: $p ===="
              kubectl -n "$NS" describe pod "$p" | tail -n +1 || true
            done
          } || echo "Indexer pods are not crashing."

      - name: Start Dashboard port-forward (auto)
        if: ${{ inputs.enable_port_forward == 'true' }}
        shell: bash
        env:
          PF_BIND: 127.0.0.1
          PF_PORT: ${{ inputs.local_forward_port }}
        run: |
          set -euo pipefail
          TARGET_PORT="${{ steps.dash.outputs.dash_port }}"
          (pkill -f "port-forward .*:${PF_PORT}:" && sleep 1) || true
          nohup kubectl -n "${NS}" port-forward --address "${PF_BIND}" deploy/wazuh-dashboard "${PF_PORT}:${TARGET_PORT}" > portforward.log 2>&1 &
          echo "::notice title=Browse Now::Open http://localhost:${PF_PORT}"

      - name: Final status
        shell: bash
        run: |
          kubectl get sc -o wide
          kubectl -n "$NS" get pvc
          kubectl -n "$NS" get svc -o wide
          kubectl -n "$NS" get endpoints -o wide
          kubectl -n "$NS" get pods -o wide
